# src/tools/documentation_search_tool.py
import os
import torch
import threading
from smolagents import Tool
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from configs import models, settings

# --- å…¨å±€ã€è¿›ç¨‹å”¯ä¸€çš„èµ„æº ---
embedding_model_instance = None
vector_store_instance = None
# ä½¿ç”¨çº¿ç¨‹é”æ¥ç¡®ä¿èµ„æºåˆå§‹åŒ–çš„åŸå­æ€§
resource_lock = threading.Lock()

def get_retriever():
    """
    ä¸€ä¸ªçº¿ç¨‹å®‰å…¨çš„å‡½æ•°ï¼Œç”¨äºè·å–å…¨å±€å”¯ä¸€çš„ retriever å®ä¾‹ã€‚
    æ‰€æœ‰å¹¶å‘çš„ Agent éƒ½ä¼šè°ƒç”¨è¿™ä¸ªå‡½æ•°ã€‚
    """
    global embedding_model_instance, vector_store_instance
    
    # ä½¿ç”¨åŒé‡æ£€æŸ¥é”å®šæ¨¡å¼ï¼Œé¿å…æ¯æ¬¡éƒ½è·å–é”ï¼Œæé«˜æ€§èƒ½
    if vector_store_instance is None:
        with resource_lock:
            # å†æ¬¡æ£€æŸ¥ï¼Œé˜²æ­¢åœ¨ç­‰å¾…é”çš„è¿‡ç¨‹ä¸­å…¶ä»–çº¿ç¨‹å·²ç»åˆå§‹åŒ–å®Œæ¯•
            if vector_store_instance is None:
                print("--- ğŸ§  Initializing Shared RAG Resources (ONCE) ---")
                
                # 1. åŠ è½½ Embedding Model
                print("Lazy loading embedding model for RAG tool...")
                device = 'cuda' if torch.cuda.is_available() else 'cpu'
                print(f"Using device: {device}")
                
                embedding_model_instance = HuggingFaceEmbeddings(
                    model_name=models.EMBEDDING_MODEL,
                    model_kwargs={'device': device},
                    encode_kwargs={'normalize_embeddings': True}
                )
                print("Embedding model loaded.")

                # 2. åŠ è½½ Vector Store
                if not os.path.exists(settings.VECTOR_DB_PATH):
                    raise FileNotFoundError(
                        f"Vector DB not found at {settings.VECTOR_DB_PATH}. "
                        "Run 'python scripts/build_vector_db.py' first."
                    )
                print("Lazy loading vector store for RAG tool...")
                vector_store_instance = FAISS.load_local(
                    settings.VECTOR_DB_PATH, 
                    embedding_model_instance, 
                    allow_dangerous_deserialization=True
                )
                print("Vector store loaded.")
                print("--- ğŸ§  Shared RAG Resources Initialized ---")

    return vector_store_instance.as_retriever(search_kwargs={"k": 3})

class DocumentationSearchTool(Tool):
    name = "documentation_search"
    description = "Searches schemdraw docs for errors or API usage questions."
    inputs = {"query": {"type": "string", "description": "Error message or API question."}}
    output_type = "string"

    def __init__(self):
        super().__init__()
        # åœ¨å·¥å…·å®ä¾‹åŒ–æ—¶ï¼Œä¸åŠ è½½ä»»ä½•ä¸œè¥¿ï¼Œåªæ˜¯ç¡®ä¿å‡½æ•°å¯ç”¨
        self.retriever = None

    def forward(self, query: str) -> str:
        # åªæœ‰åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶ï¼Œæ‰é€šè¿‡çº¿ç¨‹å®‰å…¨çš„å‡½æ•°è·å– retriever
        if self.retriever is None:
            self.retriever = get_retriever()
            
        print(f"Executing RAG search with query: '{query}'")
        results = self.retriever.invoke(query)
        
        if not results:
            return "No relevant documentation found."
            
        formatted_results = "\\n\\n---\\n\\n".join(
            [f"Source: {doc.metadata.get('source', 'N/A')}\\n\\n{doc.page_content}" for doc in results]
        )
        print("RAG search complete. Returning snippets to agent.")
        return f"Found relevant documentation snippets:\\n\\n{formatted_results}"